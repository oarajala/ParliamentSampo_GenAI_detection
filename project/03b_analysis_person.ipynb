{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d86c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages, declare constants, get the parent directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import decimal\n",
    "import numpy as np\n",
    "#from scipy import stats\n",
    "#from wordcloud import WordCloud\n",
    "#import matplotlib.pyplot as plt\n",
    "#from .utils import helpers\n",
    "#import simplemma\n",
    "#import pyvoikko\n",
    "\n",
    "CHATGPT_RELEASE_YEAR = int(2022)\n",
    "FINNISH_ALPHABET = 'abcdefghijklmnopqrstuvwxyzåäö'\n",
    "\n",
    "def get_parent_directory() -> str:\n",
    "    \"\"\"Get the parent directory for handling csv files.\n",
    "\n",
    "    Returns:\n",
    "        string: the path to the directory where directories for csv files are located\n",
    "    \"\"\"\n",
    "    #create relative path for parent\n",
    "    relative_parent = os.path.join(os.getcwd(), '..')\n",
    "\n",
    "    #use abspath for absolute parent path\n",
    "    return str(os.path.abspath(relative_parent)).replace('\\\\', '/')\n",
    "\n",
    "directory = get_parent_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea30949",
   "metadata": {},
   "source": [
    "DECLARE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcda5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string: str) -> str:\n",
    "    try:\n",
    "        # remove blanks in start and end\n",
    "        string = string.strip()\n",
    "        string = string.lower()\n",
    "        # the string must contain characters\n",
    "        if any(c in string for c in FINNISH_ALPHABET)==False:\n",
    "            string = ''\n",
    "        # remove tabulations, line breaks etc., also special characters\n",
    "        remove_these = r'[\\+\\*!\"”?.,…()§\\'[\\] \\t\\n\\r\\f\\v]'\n",
    "        string = re.sub(remove_these, '', string)\n",
    "        # remove weird parentheses and backwards linebreaks from starts of strings\n",
    "        string = re.sub(r'^\\)\\\\[a-z]', '', string)\n",
    "        # remove weird '\\[alphabet]' strings at start of strings\n",
    "        string = re.sub(r'^\\\\[a-z]', '', string)\n",
    "        # remove numbers\n",
    "        string = re.sub(r'[0-9]', '', string)\n",
    "        # remove dashes '-' at the start and end of string\n",
    "        string = re.sub(r'^-|-$', '', string)\n",
    "        # remove individual forward and backward slashes '/', '\\'\n",
    "        string = re.sub(r'[\\/\\\\]', '', string)\n",
    "        # remove double dashes '--'\n",
    "        string = string.replace('--', '-')\n",
    "        # remove the equal sign '='\n",
    "        string = string.replace('=', '')\n",
    "        # at the end of the cleaning, remove all characters from the string which are not in the alphabet except for dash (compound words)\n",
    "        remove_these = ''.join([str(c) for c in string if c != '-' and c not in [i for i in FINNISH_ALPHABET]])\n",
    "        string = re.sub(remove_these, '', string)\n",
    "        # remove blanks in start and end again\n",
    "        string = string.strip()\n",
    "        # remove empty if string length < 2\n",
    "        string = '' if len(string) < 2 else string\n",
    "        return string\n",
    "    except:\n",
    "        print(f'Unexpected error at helpers.clean_string(), string: {string}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3354488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freqs_in_string(string: str):\n",
    "    \"\"\"Counts the words in the input string.\n",
    "    Returns a dictionary where the word is the key and the frequency is the value.\n",
    "    \"\"\"\n",
    "    if ((string is None) or (string == 'nan')):\n",
    "        return None\n",
    "    else:\n",
    "        words_list = re.split(' ', string)\n",
    "        wordfreq_dict = {}\n",
    "        for word in words_list:\n",
    "            if word not in wordfreq_dict.keys():\n",
    "                wordfreq_dict[word] = 1\n",
    "            else:\n",
    "                wordfreq_dict[word] += 1\n",
    "\n",
    "        return wordfreq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0dde67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalised(df: pd.DataFrame, speaker_id: str, year: int, word_n: int) -> float:\n",
    "    w_min = df['word_n'].loc[(df['speaker_id']==speaker_id)&(df['year']==year)].min()\n",
    "    w_max = df['word_n'].loc[(df['speaker_id']==speaker_id)&(df['year']==year)].max()\n",
    "    return ((word_n-w_min)/(w_max-w_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "662ec8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lemmatised csvs for years 2015-2025\n",
    "# the goal is to check if word frequencies per SPEAKER change over years\n",
    "\n",
    "df = pd.read_csv(f'{directory}/csv_lemmatized/speeches_2015.csv', sep=';', encoding='utf-8', header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6654f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean 'speaker_id' column\n",
    "\n",
    "df['speaker_id'] = df.apply(lambda x: str(x['speaker_id']).strip() if ((x['speaker_id'] is not None) & (x['speaker_id'] is not np.nan)) else x['speaker_id'], axis=1)\n",
    "df['speaker_id'] = df.apply(lambda x: '' if x['speaker_id']=='nan' else x['speaker_id'], axis=1)\n",
    "# df['speaker_id'].loc[(df['speaker_id'].notna()==True)&(df['speaker_id'].str.len()>0)].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99bbd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data in loops: \n",
    "## > loop the csvs for years 2015-2025\n",
    "## > extract each word per speaker per df (=year), this is done in a for loop\n",
    "## > combine them all into dataframe df_speaker_words_year\n",
    "# declare the years\n",
    "year, max_year = 2015, 2025\n",
    "# start looping the csvs\n",
    "while year <= max_year:\n",
    "    # name for saving the file\n",
    "    save_file_name = f'speaker_words_{year}.csv'\n",
    "    if save_file_name in os.listdir(f'{directory}/csv_analysis/'):\n",
    "        pass\n",
    "    else:\n",
    "        # get the csv to match the year from directory: directory/csv_lemmatized/\n",
    "        year_csv = pd.read_csv(f'{directory}/csv_lemmatized/speeches_{year}.csv', sep=';', header=0)\n",
    "        # format a dataframe to store the results\n",
    "        # > columns: speaker_id, year, word, word_n (how many times the word appears)\n",
    "        df_speaker_words_year = pd.DataFrame(columns=['speaker_id', 'year', 'word', 'word_n']).astype({'speaker_id': str, 'year': int, 'word': str, 'word_n': int})    \n",
    "        # extract each word per speaker per df (=year)\n",
    "        for speaker in year_csv['speaker_id'].loc[(year_csv['speaker_id'].notna()==True)&(year_csv['speaker_id'].str.len()>0)].unique():\n",
    "            # extract only rows for the speaker in iteration, and only if a lemmatized speech exists, and only if the lemmatized speech is longer than 0 chars\n",
    "            df_filtered = year_csv.loc[(year_csv['speaker_id']==speaker)&(year_csv['content_lemmatized'].notna()==True)&(year_csv['content_lemmatized'].str.len()>0)]\n",
    "            # extract each lemmatized speech; compile them into a single string\n",
    "            speaker_all_speeches = ' '.join(df_filtered['content_lemmatized'])\n",
    "            # string cleaning: clean special characters from the string\n",
    "            speaker_all_speeches = ' '.join([clean_string(word) for word in speaker_all_speeches.split(' ')])\n",
    "            # get the count of each word in the string, return a dict\n",
    "            speaker_words_dict = count_word_freqs_in_string(speaker_all_speeches)\n",
    "            # delete empty '' keys (words) from the dict, if such have made it there. these are failed lemmatizations\n",
    "            try:\n",
    "                del speaker_words_dict['']\n",
    "            except KeyError:\n",
    "                pass\n",
    "            # add the speaker's subset (speaker_id, year, word, word_n) in the combination dataframe\n",
    "            # > from the speaker_words_dict, and year, and speaker\n",
    "            # > create another \"temporary\" dataframe for this, concatenate this to the df_speaker_words_year df\n",
    "            concat_df = pd.DataFrame.from_dict(data=speaker_words_dict, orient='index', columns=['word_n'])\n",
    "            concat_df['speaker_id'], concat_df['year'], concat_df['word'] = speaker, year, concat_df.index\n",
    "            concat_df.reset_index(drop=True, inplace=True)\n",
    "            concat_df = concat_df[['speaker_id','year','word','word_n']]  \n",
    "            df_speaker_words_year = pd.concat([df_speaker_words_year, concat_df], axis=0, ignore_index=True)\n",
    "        # store the data in a csv (savepoint!)\n",
    "        # > directory and file name template: directory/csv_analysis/speaker_words_YYYY.csv\n",
    "        df_speaker_words_year.to_csv(f'{directory}/csv_analysis/{save_file_name}', sep=';', header=True, index=False, encoding='utf-8')\n",
    "        # time for the next year\n",
    "    year = year+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "81d0b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for words per speaker for years 2015-2025 has now been created\n",
    "# next: pick the speakers who appear in all datasets\n",
    "# > speakers who are present in all datasets 2015-2025\n",
    "df_speaker_words_comp = pd.DataFrame(columns=['speaker_id', 'year', 'word', 'word_n', 'word_norm']).astype({'speaker_id':str, 'year':int, 'word':str, 'word_n':int, 'word_norm':float})\n",
    "for f in [file for file in os.listdir(f'{directory}/csv_analysis/') if re.search(r'speaker_words_\\d+\\.csv', file)]:\n",
    "    df = pd.read_csv(f'{directory}/csv_analysis/{f}', sep=';', header=0, encoding='utf-8', dtype={'speaker_id': str, 'year': int, 'word': str, 'word_n': int})\n",
    "    # calculate the normalised frequency of word per speaker per year: min-max normalisation\n",
    "    # store in column: word_norm\n",
    "    # running this in one apply with subqueries into the dataframe causes setting with copy warnings and takes way too long due to relatively large row counts\n",
    "    # > therefore let's do this step by step with fewer repetitive queries\n",
    "    df['word_norm'] = None\n",
    "    # loop through each speaker\n",
    "    for speaker in df['speaker_id'].unique():\n",
    "        # minimum and maximum word counts per speaker\n",
    "        w_min, w_max = df['word_n'].loc[df['speaker_id']==speaker].min(), df['word_n'].loc[df['speaker_id']==speaker].max()\n",
    "        # calculate the normalised frequency per each word of the speaker\n",
    "        df.loc[df['speaker_id']==speaker, 'word_norm'] = df.apply(lambda x: (x['word_n']-w_min)/(w_max-w_min), axis=1)\n",
    "    df_speaker_words_comp = pd.concat([df_speaker_words_comp, df], axis=0, ignore_index=True)\n",
    "\n",
    "# save as csv\n",
    "file_path_write = f'{directory}/csv_analysis/speaker_words_comp.csv'\n",
    "try:\n",
    "    df_speaker_words_comp.to_csv(file_path_write, sep=';', header=True, index=False, encoding='utf-8')\n",
    "except FileExistsError:\n",
    "    os.remove(file_path_write)\n",
    "    df_speaker_words_comp.to_csv(file_path_write, sep=';', header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ec772c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_words_comp = pd.read_csv(f'{directory}/csv_analysis/speaker_words_comp.csv',sep=';',header=0,encoding='utf-8',dtype={'speaker_id': str, 'year': int, 'word': str, 'word_n': int, 'word_norm':float})\n",
    "# keep only records where the speaker_id is a valid identifier (string of numbers)\n",
    "df_speaker_words_comp = df_speaker_words_comp.loc[df_speaker_words_comp['speaker_id'].str.contains(pat=r'\\d+')==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1198ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_extrapolation(y: list, x: list, n=1) -> list:\n",
    "    \"\"\"Linear extrapolation based on last two x and y observations. Returns the extrapolated value of y for a given x based on y1x1 and y2x2.\n",
    "    Parameter x will be standardised to a running sequence of numbers so extrapolations works on a linear scale.\n",
    "    Args:\n",
    "        y (list): list of values\n",
    "        x (list): list of values\n",
    "        n (int, optional): for how many times shall extrapolation be done. Defaults to 1. Larger values will start extrapolating on extrapolated values.\n",
    "\n",
    "    Returns:\n",
    "        list: extrapolated value(s) of y. The length of the list will be n.\n",
    "    \"\"\"\n",
    "    # x and y must be arrays of same length\n",
    "    # CHANGE THIS TO ASSERT\n",
    "    if len(y) != len(x):\n",
    "        print('array length mismatch')\n",
    "        return None\n",
    "    else:\n",
    "        # format helper parameters to not modify lists outside the function\n",
    "        xx = [*[i for i in x]]\n",
    "        yy = [*[i for i in y]]\n",
    "        return_list = [] # format list to be returned\n",
    "        # loop n times -> return list of n length with n extrapolations\n",
    "        # note: extrapolating on extrapolations if n>1\n",
    "        while n >= 1:\n",
    "            # format xx: it shall take a running sequence of numbers as its values\n",
    "            xx = [i for i in range(len(x))]\n",
    "            m = (yy[-1] - yy[-2]) / (xx[-1] - xx[-2])\n",
    "            y_v = yy[-2] + m * ((xx[-1] + 1) - xx[-2])\n",
    "            xx.append([xx[-1]+1])\n",
    "            yy.append(y_v)\n",
    "            return_list.append(y_v)\n",
    "            n = n-1\n",
    "        return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_words_comp['word_norm_extrap'] = None\n",
    "years = sorted(df_speaker_words_comp['year'].unique().tolist())\n",
    "years_antegpt = sorted([y for y in years if y <= CHATGPT_RELEASE_YEAR])\n",
    "# compute extrapolated values of word frequencies only for this range of years\n",
    "loop_years = [2023, 2024, 2025]\n",
    "for year in loop_years:\n",
    "    # a list to pass into linear extrapolation as x: two previous years upon which to extrapolate\n",
    "    years_extrap = [year-2, year-1]\n",
    "    speakers_year = df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==year].unique().tolist()\n",
    "    for speaker in speakers_year:\n",
    "        # if the speaker is not in the sets of the previous two years -> skip processing the speaker's words\n",
    "        # linear extrapolation would return None for the speaker and processing takes a lot of time\n",
    "        if (speaker not in df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==year-1].unique()) and (speaker not in df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==year-2].unique()):\n",
    "            pass\n",
    "        else:\n",
    "            speaker_words_year = df_speaker_words_comp['word'].loc[(df_speaker_words_comp['year']==year)&(df_speaker_words_comp['speaker_id']==speaker)].unique().tolist()\n",
    "            #speaker_words_year = ['puhuja', 'ei', 'tämä']\n",
    "            # compute linear extrapolation of word\n",
    "            for word in speaker_words_year:\n",
    "                # list the normalised values of word frequencies from previous TWO years for the extrapolation\n",
    "                # > create a list for this\n",
    "                # > ARBITRARY TWO (2) YEARS: the linear extrapolation function handles only 2 years and this is done to limit the amount of looping\n",
    "                # > change this if 2 is not enough, but for the function to work 2 _is_ enough\n",
    "                list_word_norms = []\n",
    "                for y in years_extrap:\n",
    "                    list_word_norms.append(df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==y)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word)].values.astype(float))\n",
    "                print(f'year: {year} - speaker: {speaker} - word: {word} - list_word_norms: {list_word_norms}')\n",
    "                word_norm_extrap = linear_extrapolation(y=list_word_norms, x=years_extrap, n=1)[0]\n",
    "                print(f'word_norm_extrap: {word_norm_extrap} - len(word_norm_extrap): {len(word_norm_extrap)}')\n",
    "                # add the extrapolated value for the word only if such value has been created\n",
    "                if len(word_norm_extrap)==1:\n",
    "                    df_speaker_words_comp.loc[(df_speaker_words_comp['year']==year)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word), 'word_norm_extrap'] = word_norm_extrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "42d94268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_words_comp = pd.read_csv(f'{directory}/csv_analysis/speaker_words_comp.csv',sep=';',header=0,encoding='utf-8',dtype={'speaker_id': str, 'year': int, 'word': str, 'word_n': int, 'word_norm':float})\n",
    "# keep only records where the speaker_id is a valid identifier (string of numbers)\n",
    "df_speaker_words_comp = df_speaker_words_comp.loc[df_speaker_words_comp['speaker_id'].str.contains(pat=r'\\d+')==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute extrapolated values of word frequencies only for this range of years\n",
    "loop_years = [2023, 2024, 2025]\n",
    "for year in loop_years:\n",
    "    # name for a new column: extrapolated word frequency\n",
    "    # > a new column will be created for each looped year\n",
    "    col_word_norm_extrap = f'word_norm_extrap_{year}'\n",
    "    #df_speaker_words_comp[col_word_norm_extrap] = None\n",
    "    # merging the dataframe will speed this up instead of looping each row\n",
    "    # > create a new df to quickly compute the linear extrapolation for word frequency: df_merge\n",
    "    # > df_merge is created by joining df_speaker_words_comp for year, said df year-1 and year-2 on speaker_id and word\n",
    "    # > INNER JOIN filters rows where there are no normalised frequencies in the previous years -> extrapolation for these words would fail\n",
    "    # > LEFT JOIN keeps all recrods - this can be done to keep the records \n",
    "    # > finally the extrapolated normalised frequency is updated into df_speaker_words_comp in the update\n",
    "    df_merge = df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year].merge(\n",
    "                   df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-1],\n",
    "                   how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_1'))\n",
    "    df_merge = df_merge.merge(df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-2],\n",
    "        how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_2'))\n",
    "    df_merge[col_word_norm_extrap] = df_merge.apply(lambda x: linear_extrapolation(y=[x['word_norm_t_2'], x['word_norm_t_1']],x=[year-2, year-1],n=1)[0], axis=1)\n",
    "    df_speaker_words_comp = df_speaker_words_comp.merge(df_merge[['speaker_id','word','year',col_word_norm_extrap]], how='left', on=['speaker_id','word','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973e7ef",
   "metadata": {},
   "source": [
    "Test if the getting extrapolations for years 2023, 2024 and 2025 worked.\n",
    "\n",
    "```\n",
    "df_speaker_words_comp.loc[(df_speaker_words_comp['speaker_id']=='1126')&(df_speaker_words_comp['word']=='viikko')]\n",
    "```\n",
    "\n",
    "Next collapse word_norm_extrap_[2023, 2024, 2025] into a single column, word_norm_extrap, since year is already a column in the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d60334c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_words_comp.loc[df_speaker_words_comp['year']==2023, 'word_norm_extrap'] = df_speaker_words_comp['word_norm_extrap_2023'] \n",
    "df_speaker_words_comp.loc[df_speaker_words_comp['year']==2024, 'word_norm_extrap'] = df_speaker_words_comp['word_norm_extrap_2024'] \n",
    "df_speaker_words_comp.loc[df_speaker_words_comp['year']==2025, 'word_norm_extrap'] = df_speaker_words_comp['word_norm_extrap_2025']\n",
    "# keep only columns speaker_id, year, word, word_n, word_norm, word_norm_extrap\n",
    "df_speaker_words_comp = df_speaker_words_comp[['speaker_id','year','word','word_n','word_norm','word_norm_extrap']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3741842",
   "metadata": {},
   "source": [
    "Validate that collapsing the columns worked.\n",
    "\n",
    "<code>df_speaker_words_comp.loc[(df_speaker_words_comp['speaker_id']=='1126')&(df_speaker_words_comp['word']=='viikko')]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9359ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>year</th>\n",
       "      <th>word</th>\n",
       "      <th>word_n</th>\n",
       "      <th>word_norm</th>\n",
       "      <th>word_norm_extrap</th>\n",
       "      <th>word_diffs</th>\n",
       "      <th>word_ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166269</th>\n",
       "      <td>1126</td>\n",
       "      <td>2015</td>\n",
       "      <td>viikko</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320467</th>\n",
       "      <td>1126</td>\n",
       "      <td>2016</td>\n",
       "      <td>viikko</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920249</th>\n",
       "      <td>1126</td>\n",
       "      <td>2017</td>\n",
       "      <td>viikko</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500596</th>\n",
       "      <td>1126</td>\n",
       "      <td>2018</td>\n",
       "      <td>viikko</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001247</th>\n",
       "      <td>1126</td>\n",
       "      <td>2020</td>\n",
       "      <td>viikko</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606891</th>\n",
       "      <td>1126</td>\n",
       "      <td>2021</td>\n",
       "      <td>viikko</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416948</th>\n",
       "      <td>1126</td>\n",
       "      <td>2022</td>\n",
       "      <td>viikko</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668046</th>\n",
       "      <td>1126</td>\n",
       "      <td>2023</td>\n",
       "      <td>viikko</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>-0.832669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822633</th>\n",
       "      <td>1126</td>\n",
       "      <td>2024</td>\n",
       "      <td>viikko</td>\n",
       "      <td>14</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>1.222097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295542</th>\n",
       "      <td>1126</td>\n",
       "      <td>2025</td>\n",
       "      <td>viikko</td>\n",
       "      <td>21</td>\n",
       "      <td>0.016103</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>1.039471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speaker_id  year    word  word_n  word_norm  word_norm_extrap  \\\n",
       "166269        1126  2015  viikko       1   0.000000               NaN   \n",
       "320467        1126  2016  viikko       2   0.000930               NaN   \n",
       "920249        1126  2017  viikko       1   0.000000               NaN   \n",
       "1500596       1126  2018  viikko       2   0.001036               NaN   \n",
       "2001247       1126  2020  viikko       4   0.003916               NaN   \n",
       "2606891       1126  2021  viikko       5   0.004785               NaN   \n",
       "3416948       1126  2022  viikko       1   0.000000               NaN   \n",
       "3668046       1126  2023  viikko       3   0.003984         -0.004785   \n",
       "3822633       1126  2024  viikko      14   0.009738          0.007968   \n",
       "4295542       1126  2025  viikko      21   0.016103          0.015492   \n",
       "\n",
       "         word_diffs  word_ratios  \n",
       "166269          NaN          NaN  \n",
       "320467          NaN          NaN  \n",
       "920249          NaN          NaN  \n",
       "1500596         NaN          NaN  \n",
       "2001247         NaN          NaN  \n",
       "2606891         NaN          NaN  \n",
       "3416948         NaN          NaN  \n",
       "3668046    0.008769    -0.832669  \n",
       "3822633    0.001770     1.222097  \n",
       "4295542    0.000611     1.039471  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute differences and ratios between actual normalised frequencies and the extrapolated values:\n",
    "# > diffs: actual value minus extrapolated value\n",
    "# > ratios: actual value divided by extrapolated value\n",
    "# >> replace extrapolated value with 1 to avoid divide by zero errors\n",
    "df_speaker_words_comp['word_diffs'] = df_speaker_words_comp.apply(lambda x: x['word_norm'] - x['word_norm_extrap'], axis=1)\n",
    "df_speaker_words_comp['word_ratios'] = df_speaker_words_comp.apply(lambda x: x['word_norm'] / (1 if (x['word_norm_extrap'] is None or x['word_norm_extrap']==np.float64(0)) else x['word_norm_extrap']), axis=1)\n",
    "df_speaker_words_comp.loc[(df_speaker_words_comp['speaker_id']=='1126')&(df_speaker_words_comp['word']=='viikko')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "91105ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1147', '947', '1392', '1126', '1430', '784', '1385', '1310',\n",
       "       '1157', '464', '1400', '1325', '1380', '1328', '1149', '1422',\n",
       "       '1334', '1424', '1106', '1387', '1306', '1384', '1388', '1089',\n",
       "       '797', '1429', '1437', '778', '1454', '1417', '1383', '967',\n",
       "       '1409', '1133', '301', '1418', '1403', '1137', '1408', '1265',\n",
       "       '1301', '1411', '1129', '1449', '1141', '511', '1135', '1450',\n",
       "       '1331', '583', '1298', '1312', '1300', '1302', '612', '1326',\n",
       "       '1131', '1382', '1094', '1452', '1096', '1455', '1282', '1299',\n",
       "       '971', '1451', '1349', '351', '1396', '953', '358', '1445', '1345',\n",
       "       '1401', '1426', '1097', '538', '1099', '1468', '1340', '1402',\n",
       "       '1440', '1391', '1447', '1483', '1219', '499', '1341', '1398',\n",
       "       '1323', '1469', '1443', '1435', '1144', '1415', '960', '1338',\n",
       "       '963', '1390', '1100', '1439', '1410', '1134', '1093', '772',\n",
       "       '1327', '1308', '1433', '1318', '794', '1428', '1379', '970',\n",
       "       '1314', '1441', '1413', '941', '1425', '926', '1155', '1394',\n",
       "       '1419', '1434', '1136', '1279', '632', '1412', '1316', '1481',\n",
       "       '1432', '903', '1389', '1142', '118'], dtype=object)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speaker_words_comp['speaker_id'].loc[(df_speaker_words_comp['year']==2024)&(df_speaker_words_comp['word_ratios']>5)].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(df_speaker_words_comp['year'].unique().tolist())\n",
    "years_antegpt = sorted([y for y in years if y <= CHATGPT_RELEASE_YEAR])\n",
    "#df_speaker_words_comp['word_norm_extrap'] = None\n",
    "# compute extrapolated values of word frequencies only for this range of years\n",
    "loop_years = [2023, 2024, 2025]\n",
    "for year in loop_years:\n",
    "    # a list to pass into linear extrapolation as x: two previous years upon which to extrapolate\n",
    "    years_extrap = [year-2, year-1]\n",
    "    # name for a new column: extrapolated word frequency\n",
    "    # > a new column will be created for each looped year\n",
    "    #col_word_norm_extrap = f'word_norm_extrap_{year}'\n",
    "    #df_speaker_words_comp[col_word_norm_extrap] = None\n",
    "    # merging the dataframe will speed this up instead of looping each row\n",
    "    # > create a new df to quickly compute the linear extrapolation for word frequency: df_merge\n",
    "    # > df_merge is created by joining df_speaker_words_comp for year, said df year-1 and year-2 on speaker_id and word\n",
    "    # > INNER JOIN filters rows where there are no normalised frequencies in the previous years -> extrapolation for these words would fail\n",
    "    # > LEFT JOIN keeps all recrods - this can be done to keep the records \n",
    "    # > finally the extrapolated normalised frequency is updated into df_speaker_words_comp in the update\n",
    "    df_merge = df_speaker_words_comp[['speaker_id','year','word','word_norm']].merge(\n",
    "                    df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-1],\n",
    "                    how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_1'))\n",
    "#   df_merge = df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year].merge(\n",
    "#                   df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-1],\n",
    "#                   how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_1'))\n",
    "    df_merge = df_merge.merge(df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-2],\n",
    "                    how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_2'))\n",
    "    df_merge['word_norm_extrap'] = df_merge.apply(lambda x: linear_extrapolation(y=[x['word_norm_t_2'], x['word_norm_t_1']],x=years_extrap,n=1)[0], axis=1)\n",
    "    df_speaker_words_comp = df_speaker_words_comp.merge(df_merge[['speaker_id','word','year','word_norm_extrap']], how='left', on=['speaker_id','word','year'])\n",
    "    #df_speaker_words_comp[['speaker_id','word','year','word_norm_extrap']].update(df_merge[['speaker_id','word','year','word_norm_extrap']])\n",
    "    #df_speaker_words_comp.loc[df_speaker_words_comp['year']==year].update(df_merge[['speaker_id','word','year','word_norm_extrap']])\n",
    "    #df_speaker_words_comp.mask(df_speaker_words_comp[['speaker_id','word','year']].isin(df_merge[['speaker_id','word','year']]), df_merge['word_norm_extrap'], inplace=True, axis=1)\n",
    "    #df_speaker_words_comp.update(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(df_speaker_words_comp['year'].unique().tolist())\n",
    "years_antegpt = sorted([y for y in years if y <= CHATGPT_RELEASE_YEAR])\n",
    "df_speaker_words_comp['word_norm_extrap'] = None\n",
    "# compute extrapolated values of word frequencies only for this range of years\n",
    "loop_years = [2023, 2024, 2025]\n",
    "for year in loop_years:\n",
    "    # a list to pass into linear extrapolation as x: two previous years upon which to extrapolate\n",
    "    years_extrap = [year-2, year-1]\n",
    "    # name for a new column: extrapolated word frequency\n",
    "    # > a new column will be created for each looped year\n",
    "    #col_word_norm_extrap = f'word_norm_extrap_{year}'\n",
    "    #df_speaker_words_comp[col_word_norm_extrap] = None\n",
    "    # merging the dataframe will speed this up instead of looping each row\n",
    "    # > create a new df to quickly compute the linear extrapolation for word frequency: df_merge\n",
    "    # > df_merge is created by joining df_speaker_words_comp for year, said df year-1 and year-2 on speaker_id and word\n",
    "    # > INNER JOIN filters rows where there are no normalised frequencies in the previous years -> extrapolation for these words would fail\n",
    "    # > LEFT JOIN keeps all recrods - this can be done to keep the records \n",
    "    # > finally the extrapolated normalised frequency is updated into df_speaker_words_comp in the update\n",
    "    df_merge = df_speaker_words_comp[['speaker_id','year','word','word_norm']].merge(\n",
    "                    df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-1],\n",
    "                    how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_1'))\n",
    "#   df_merge = df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year].merge(\n",
    "#                   df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-1],\n",
    "#                   how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_1'))\n",
    "    df_merge = df_merge.merge(df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[df_speaker_words_comp['year']==year-2],\n",
    "                    how='left', on=['speaker_id', 'word'], suffixes=(None,'_t_2'))\n",
    "    df_merge['word_norm_extrap'] = df_merge.apply(lambda x: linear_extrapolation(y=[x['word_norm_t_2'], x['word_norm_t_1']],x=years_extrap,n=1)[0], axis=1)\n",
    "    #df_speaker_words_comp = df_speaker_words_comp.merge(df_merge[['speaker_id','word','year','word_norm_extrap']], how='left', on=['speaker_id','word','year'])\n",
    "    #df_speaker_words_comp[['speaker_id','word','year','word_norm_extrap']].update(df_merge[['speaker_id','word','year','word_norm_extrap']])\n",
    "    #df_speaker_words_comp.loc[df_speaker_words_comp['year']==year].update(df_merge[['speaker_id','word','year','word_norm_extrap']])\n",
    "    #df_speaker_words_comp.mask(df_speaker_words_comp[['speaker_id','word','year']].isin(df_merge[['speaker_id','word','year']]), df_merge['word_norm_extrap'], inplace=True, axis=1)\n",
    "    df_speaker_words_comp.update(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca85025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '1126'\n",
    "#print(speaker_words_year)\n",
    "#print(list_word_norms)\n",
    "#list_word_norms = []\n",
    "#.append(df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==y)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word)].values)\n",
    "#print(list_word_norms)\n",
    "#print(year)\n",
    "#print(y)\n",
    "#print(speaker)\n",
    "#print(word)\n",
    "#rint(list_word_norms, years_extrap)\n",
    "#print(word_norm_extrap)\n",
    "#df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==2023)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word)]\n",
    "#df_speaker_words_comp['word'].loc[(df_speaker_words_comp['year']==2023)&(df_speaker_words_comp['speaker_id']=='1503')&(df_speaker_words_comp['word']=='ja')]\n",
    "#df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2023]\n",
    "#test_list = []\n",
    "#test_list.append(df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==2023)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word)].values)\n",
    "#print(test_list)\n",
    "#df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==2023)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word)].values[0].astype(float)\n",
    "#df_speaker_words_comp['year'].unique()\n",
    "#re.search(pattern=r'\\d+', string=df_speaker_words_comp['speaker_id'].unique())\n",
    "#df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['speaker_id'].str.contains(pat=r'\\d+')==False].unique()\n",
    "#df_speaker_words_comp.loc[df_speaker_words_comp['word_norm_extrap'].notna()==True]\n",
    "# 1096\n",
    "df_merge = df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[(df_speaker_words_comp['year']==2023)&(df_speaker_words_comp['speaker_id']=='1096')].merge(\n",
    "         df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[(df_speaker_words_comp['year']==2022)&(df_speaker_words_comp['speaker_id']=='1096')],\n",
    "         how='inner', on=['speaker_id', 'word'], suffixes=(None,'_t_1'))\n",
    "df_merge = df_merge.merge(df_speaker_words_comp[['speaker_id','year','word','word_norm']].loc[(df_speaker_words_comp['year']==2021)&(df_speaker_words_comp['speaker_id']=='1096')],\n",
    "         how='inner', on=['speaker_id', 'word'], suffixes=(None,'_t_2'))\n",
    "df_merge['word_norm_extrap'] = df_merge.apply(lambda x: linear_extrapolation(y=[x['word_norm_t_2'], x['word_norm_t_1']],x=[2021,2022],n=1)[0], axis=1)\n",
    "df_speaker_words_comp[['speaker_id','year','word','word_norm_extrap']].loc[(df_speaker_words_comp['year']==2023)&(df_speaker_words_comp['speaker_id']=='1096')].update(df_merge[['speaker_id','year','word','word_norm_extrap']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74f27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>year</th>\n",
       "      <th>word</th>\n",
       "      <th>word_n</th>\n",
       "      <th>word_norm</th>\n",
       "      <th>word_norm_extrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [speaker_id, year, word, word_n, word_norm, word_norm_extrap]\n",
       "Index: []"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_speaker_words_comp.loc[df_speaker_words_comp['word_norm_extrap'].notna()==True]\n",
    "#df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==y)&(df_speaker_words_comp['speaker_id']==speaker)&(df_speaker_words_comp['word']==word)]\n",
    "#df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==y)&(df_speaker_words_comp['speaker_id']==speaker)]\n",
    "#df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['year']==y)]\n",
    "#df_speaker_words_comp['word_norm'].loc[(df_speaker_words_comp['speaker_id']==speaker)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ade1ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker_words_comp= df_speaker_words_comp[df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2015])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2016])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2017])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2018])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2019])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2020])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2021])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2022])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2023])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2024])&\n",
    "   df_speaker_words_comp['speaker_id'].isin(df_speaker_words_comp['speaker_id'].loc[df_speaker_words_comp['year']==2025])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the normalised frequency of word per speaker per year: min-max normalisation\n",
    "# store in column: word_norm\n",
    "df_speaker_words_comp['word_norm'] = None\n",
    "for i, d in df_speaker_words_comp.iterrows():\n",
    "    w_min = df_speaker_words_comp['word_n'].loc[(df_speaker_words_comp['speaker_id']==d.speaker_id)&(df_speaker_words_comp['year']==d.year)].min()\n",
    "    w_max = df_speaker_words_comp['word_n'].loc[(df_speaker_words_comp['speaker_id']==d.speaker_id)&(df_speaker_words_comp['year']==d.year)].max()\n",
    "    # (x[col]-col_min)/(col_max-col_min)\n",
    "    d.word_norm = (d.word_n-w_min)/(w_max-w_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb31554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(635)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speaker_words_comp['word_n'].loc[(df_speaker_words_comp['speaker_id']=='1096')&(df_speaker_words_comp['year']==2015)].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b6386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalised(speaker_id: str, year: int, word_n: int) -> float:\n",
    "    w_min = df_speaker_words_comp['word_n'].loc[(df_speaker_words_comp['speaker_id']==speaker_id)&(df_speaker_words_comp['year']==year)].min()\n",
    "    w_max = df_speaker_words_comp['word_n'].loc[(df_speaker_words_comp['speaker_id']==speaker_id)&(df_speaker_words_comp['year']==year)].max()\n",
    "    return ((word_n-w_min)/(w_max-w_min))\n",
    "\n",
    "df_speaker_words_comp['word_norm'] = None\n",
    "df_speaker_words_comp['word_norm'] = df_speaker_words_comp.apply(lambda x: get_normalised(x['speaker_id'], x['year'], x['word_n']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27690ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[df['speaker_id']=='127']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34c716de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMISTAJA\\AppData\\Local\\Temp\\ipykernel_19536\\440395192.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['word_norm'] = test_df.apply(lambda x: get_normalised(test_df, x['speaker_id'], x['year'], x['word_n']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test_df['word_norm'] = test_df.apply(lambda x: get_normalised(test_df, x['speaker_id'], x['year'], x['word_n']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e6c3828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>year</th>\n",
       "      <th>word</th>\n",
       "      <th>word_n</th>\n",
       "      <th>word_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>127</td>\n",
       "      <td>2015</td>\n",
       "      <td>olla</td>\n",
       "      <td>207</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speaker_id  year  word  word_n  word_norm\n",
       "30        127  2015  olla     207        1.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['word_norm']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa4cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
